{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install transformers\n",
    "%pip install bitsandbytes\n",
    "%pip install accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a basic Hello World test. I have GPU with only 11 GB of RAM, so I want to do 8 bit quantisation for gemma2 2b model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-2-2b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load quantinised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae968a3ad944dcc80f7aeb40d22ac06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the data type for computations to float16, bfloat16 not supported on T4/P100\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "# Configure the BitsAndBytes settings for 8-bit quantization to reduce memory usage\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,  # Enable 8-bit quantization\n",
    "    )\n",
    "\n",
    "# Load the pre-trained model with specified configurations\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,  # Apply the 4-bit quantization configuration\n",
    "        torch_dtype=compute_dtype,  # Set the data type for the model\n",
    "        use_cache=False,  # Disable caching to save memory\n",
    "        device_map='auto',  # Automatically map the model to available devices (e.g., GPUs)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear8bitLt(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=2048, out_features=2304, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       "  (_cache): HybridCache()\n",
       ")"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "#tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build interference pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Gemma, an AI assistant created by the Gemma team. I'm an open-weights large language model, which means I'm publicly available. I can generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way. \n",
      "\n",
      "What can I help you with today? ðŸ˜Š \n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"Who are you?\"},\n",
    "]\n",
    "response = pipe(messages,\n",
    "     max_new_tokens=1024,\n",
    "     do_sample=True,\n",
    "     #eos_token_id=terminators,\n",
    "     temperature=0.0001)\n",
    "\n",
    "print(response[0]['generated_text'][1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyse in more detail how the model works. See first spcecial tokens from tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<bos>': 2,\n",
       " '<eos>': 1,\n",
       " '<unk>': 3,\n",
       " '<pad>': 0,\n",
       " '<start_of_turn>': 106,\n",
       " '<end_of_turn>': 107}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_special_token_map = {}\n",
    "for key in tokenizer.special_tokens_map:\n",
    "  if key != 'additional_special_tokens':\n",
    "    decoded_token = tokenizer.special_tokens_map[key]\n",
    "    raw_special_token_map[decoded_token] = tokenizer.convert_tokens_to_ids(decoded_token)\n",
    "  else:\n",
    "    for decoded_token in tokenizer.special_tokens_map['additional_special_tokens']:\n",
    "      raw_special_token_map[decoded_token] = tokenizer.convert_tokens_to_ids(decoded_token)\n",
    "\n",
    "raw_special_token_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer encodes sequence of words into tokens\n",
    "v = tokenizer.encode(\"2+4=?\")\n",
    "input_tensor = torch.tensor(v).reshape(1, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2, 235284, 235340, 235310,  61395]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model generates output sequence of tokens\n",
    "outputs = model.generate(input_ids=input_tensor.cuda(), attention_mask=torch.ones(input_tensor.shape).cuda(), temperature=0.0001,  max_new_tokens=1024,\n",
    "     do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2, 235284, 235340, 235310,  61395,    109,   4858, 235303, 235256,\n",
       "           1368,    577,  11560,    665, 235292,    109, 235287,   5231,   5089,\n",
       "            675,    573,   5081,  66058, 235248, 235284,    963, 235248, 235310,\n",
       "            108, 235287,   5231,   2341,    573,   5968,   3584,  66058, 235248,\n",
       "         235284,    963, 235248, 235310,    589, 235248, 235318,    109,   2339,\n",
       "         235269, 235248, 235284,    963, 235248, 235310,    589, 235248, 235318,\n",
       "         235248,    108,    107]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because we saw special tokens, now you can recognize them and see their decoded versions in the result in the next cell\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>2+4=?\n",
      "\n",
      "Here's how to solve it:\n",
      "\n",
      "* **Start with the addition:** 2 + 4\n",
      "* **Add the numbers together:** 2 + 4 = 6\n",
      "\n",
      "So, 2 + 4 = 6 \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer decodes output sequence of tokens back into sequence of words\n",
    "print(tokenizer.decode(token_ids=outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what is a raw mathematical output of the model\n",
    "output = model(input_ids=input_tensor.cuda(), attention_mask=torch.ones(input_tensor.shape).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[-16.2188,  -5.8867,   0.3484,  ..., -11.0859, -11.1406, -16.2188],\n",
       "         [-14.4766,  -9.4766, -20.9531,  ..., -10.8828,  -8.6562, -13.4531],\n",
       "         [-13.8516,  -1.9521,  -6.4336,  ...,  -8.8984,  -7.2500, -12.0938],\n",
       "         [-15.4844,  -2.9512,  -8.2188,  ...,  -9.9922,  -7.9375, -14.1250],\n",
       "         [-10.3594,   6.2734,  -4.3945,  ...,  -9.7500,  -7.8594,  -8.3750]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<MulBackward0>), past_key_values=None, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it is a sequence of log vectors (which represent logs from token odds). \n",
    "# The length of the sequence is equal exactly to the number of input tokens, which corresponds to general knowledge about transformer architecture\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 '\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "# Let's see what is the most probable last token, which will be the first talke from the response\n",
    "# This is the same token, which we obtained from generation ealier\n",
    "\n",
    "token = int(torch.argmax(F.softmax(output.logits[:, -1, :][0], dim=0)))\n",
    "print(token, repr(tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define softmax with temperature\n",
    "\n",
    "def softmax(input, t=1.0):\n",
    "  return F.softmax(input/t, dim=0, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n",
      "5856\n",
      "235248\n",
      "235274\n",
      "235265\n",
      "4463\n",
      "573\n",
      "1758\n",
      "235248\n",
      "235284\n",
      "577\n",
      "573\n",
      "1758\n",
      "235248\n",
      "235310\n",
      "235265\n",
      "235248\n",
      "109\n",
      "5856\n",
      "235248\n",
      "235284\n",
      "235265\n",
      "714\n",
      "2707\n",
      "603\n",
      "235248\n",
      "235318\n",
      "235265\n",
      "235248\n",
      "109\n",
      "651\n",
      "3448\n",
      "603\n",
      "235248\n",
      "235318\n",
      "235265\n",
      "235248\n",
      "108\n",
      "107\n",
      "<bos>2+4=?\n",
      "\n",
      "Step 1. Add the number 2 to the number 4. \n",
      "\n",
      "Step 2. The sum is 6. \n",
      "\n",
      "The answer is 6. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's generate response token by token, choosing each time token from probability distribution given by last vector of output \n",
    "# and attaching it to the current prompt\n",
    "\n",
    "prompt = tokenizer.encode(\"2+4=?\")\n",
    "current = prompt[:-1]\n",
    "token = prompt[-1]\n",
    "end_of_turn = raw_special_token_map['<end_of_turn>']\n",
    "\n",
    "\n",
    "while token != end_of_turn:\n",
    "  current = current + [token]\n",
    "  input_tensor = torch.tensor(current).reshape(1, len(current))  \n",
    "  with torch.no_grad():\n",
    "    output = model(input_ids=input_tensor.cuda(), attention_mask=torch.ones(input_tensor.shape).cuda())\n",
    "\n",
    "  logits = output.logits[:, -1, :][0]\n",
    "  probs = softmax(logits, t=1.0)\n",
    "  token = int(torch.multinomial(probs, num_samples=1))\n",
    "  print(token)\n",
    "\n",
    "print(tokenizer.decode(current))\n",
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
